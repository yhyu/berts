{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to make your Transformer program assignment alive?\n",
    "\n",
    "This example is to train Transformer by utilizing your program assignment - [Transformers Architecture with TensorFlow](https://www.coursera.org/learn/nlp-sequence-models/programming/roP5y/transformers-architecture-with-tensorflow) in Deep Learning Specialization - Sequence Model Week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just copy what you have done in the program assignment and save to `transformer.py` (check what you should copy to [transformer.py](https://github.com/yhyu/berts/blob/master/transformer/transformer.py), this file is copy from program assignment without answer.), so that this example can import your assignment.\n",
    "```python\n",
    "from transformer import *\n",
    "```\n",
    "However, there is a little bit different in mask function. Since we are using tensorflow [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention), its padding mask definition looks like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.math.not_equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    return tf.linalg.band_part(tf.ones((size, size)), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # input mask for encoder\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # attention mask for decoder attends to encoder\n",
    "    enc_dec_attend_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # input mask for decoder (look_ahead_mask + padding_mask)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_padding_mask = create_padding_mask(tar)\n",
    "    dec_input_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, dec_input_mask, enc_dec_attend_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, modified Transformer call function arguments order to move training to the last argument and set default value None (refer to [transformer.py](https://github.com/yhyu/berts/blob/cb3a6bf8b79be639f5f3108199d6a5f3f31a1bb2/transformer/transformer.py#L360)). It looks like:\n",
    "\n",
    "```python\n",
    "class Transformer(tf.keras.Model):\n",
    "    # ...\n",
    "    def call(self, inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask, training=None):\n",
    "```\n",
    "\n",
    "Okay, let's define a function to build NMT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_layers, num_heads, embedding_dim, fully_connected_dim,\n",
    "                inp_vocab_size, tar_vocab_size, max_pos_encoding_inp, max_pos_encoding_tar):\n",
    "    # embedding layer takes word index rather than one-hot vec\n",
    "    src_input = tf.keras.Input(shape=(None,))            # source language input\n",
    "    tar_input = tf.keras.Input(shape=(None,))            # target language input\n",
    "    src_input_mask = tf.keras.Input(shape=(1, None))     # source language input mask\n",
    "    src_tar_mask = tf.keras.Input(shape=(1, None))       # target attends to source language mask\n",
    "    tar_input_mask = tf.keras.Input(shape=(None, None))  # target language input mask\n",
    "    \n",
    "    outputs, _ = Transformer(\n",
    "        num_layers,\n",
    "        embedding_dim,\n",
    "        num_heads,\n",
    "        fully_connected_dim,\n",
    "        inp_vocab_size,\n",
    "        tar_vocab_size,\n",
    "        max_pos_encoding_inp,\n",
    "        max_pos_encoding_tar\n",
    "    )(src_input, tar_input, src_input_mask, tar_input_mask, src_tar_mask)\n",
    "    return tf.keras.Model(inputs=[src_input, tar_input, src_input_mask, src_tar_mask, tar_input_mask], outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data set\n",
    "\n",
    "The remaining is tedious, but crucial in ML engineer daily life.\n",
    "\n",
    "Tensorflow has some data set for NMT, you can pick any one. I use [wmt15_translate/fr-en](https://www.tensorflow.org/datasets/catalog/wmt15_translate#wmt15_translatefr-en) data set in the example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare local file location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nmt_en-fr\"\n",
    "en_vocab_file = os.path.join(output_dir, \"en_vocab\")\n",
    "fr_vocab_file = os.path.join(output_dir, \"fr_vocab\")\n",
    "download_dir = \"tensorflow-datasets/downloads\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a lok at what corpouses we got in [wmt15_translate/fr-en](https://www.tensorflow.org/datasets/catalog/wmt15_translate#wmt15_translatefr-en) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Split('train'): ['europarl_v7',\n",
       "  'commoncrawl',\n",
       "  'multiun',\n",
       "  'newscommentary_v10',\n",
       "  'gigafren'],\n",
       " Split('validation'): ['newsdiscussdev2015', 'newstest2014'],\n",
       " Split('test'): ['newsdiscusstest2015']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.builder(\"wmt15_translate/fr-en\").subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download training and validation data set: I chose a small size corpouse, `newscommentary_v10`, you can pick a large one to train your transformer more powerful such as `commoncrawl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Using custom data configuration fr-en\n"
     ]
    }
   ],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "    version=\"0.0.3\",\n",
    "    language_pair=(\"fr\", \"en\"),\n",
    "    subsets={\n",
    "        tfds.Split.TRAIN: [\"newscommentary_v10\"],\n",
    "        tfds.Split.VALIDATION: [\"newstest2014\"],\n",
    "    },\n",
    ")\n",
    "builder = tfds.builder(\"wmt_translate\", config=config)\n",
    "builder.download_and_prepare(download_dir=download_dir)\n",
    "ds_train, ds_val = builder.as_dataset(split=['train[:]', 'validation[:]'], as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "Most of following code snippets are copy from tensorflow [Subword tokenizers](https://www.tensorflow.org/tutorials/tensorflow_text/subwords_tokenizer). It needs `tf-nightly` and `tensorflow_text_nightly` at this moment. Hopefully these features will be moved to official build soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 2**13,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocab(dataset, dsIdx, output_file, batch_size, prefetch):\n",
    "    if os.path.isfile(output_file + '.txt'):\n",
    "        vocab = pathlib.Path(output_file + '.txt').read_text(encoding='utf-8').splitlines()\n",
    "    else:\n",
    "        vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "            dataset.batch(batch_size).prefetch(prefetch),\n",
    "            **bert_vocab_args\n",
    "        )\n",
    "        \n",
    "        with open(output_file + '.txt', 'w', encoding='utf8') as f:\n",
    "            for token in vocab:\n",
    "                print(token, file=f)\n",
    "\n",
    "    print('vocabulary size of %s: %d' % (dsIdx, len(vocab)))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = ds_train.map(lambda fr, en: en)\n",
    "fr_train = ds_train.map(lambda fr, en: fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please forgive me for my lazy bone. I took all training data in one shot. It's not a good way, but ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size of 0: 7794\n",
      "vocabulary size of 1: 7978\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 50\n",
    "BATCH_SIZE = 320000 # only take 320000\n",
    "BUFFER_SIZE = 15000\n",
    "vocab_en = gen_vocab(en_train, 0, en_vocab_file, BATCH_SIZE, BUFFER_SIZE)\n",
    "vocab_fr = gen_vocab(fr_train, 1, fr_vocab_file, BATCH_SIZE, BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = text.BertTokenizer(fr_vocab_file + '.txt', **bert_tokenizer_params)\n",
    "en_tokenizer = text.BertTokenizer(en_vocab_file + '.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 3\n"
     ]
    }
   ],
   "source": [
    "PAD = tf.argmax(tf.constant(reserved_tokens) == \"[PAD]\").numpy()\n",
    "BOS = tf.argmax(tf.constant(reserved_tokens) == \"[START]\").numpy()\n",
    "EOS = tf.argmax(tf.constant(reserved_tokens) == \"[END]\").numpy()\n",
    "print(PAD, BOS, EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess training data: tokenize, truncate over MAX_LENGTH and pad less than MAX_LENGTH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(fr, en):\n",
    "    en_indices = [BOS] + list(np.squeeze(en_tokenizer.tokenize(en.numpy()).merge_dims(-2, -1).to_list(), axis=0)) + [EOS]\n",
    "    fr_indices = [BOS] + list(np.squeeze(fr_tokenizer.tokenize(fr.numpy()).merge_dims(-2, -1).to_list(), axis=0)) + [EOS]\n",
    "    return fr_indices, en_indices\n",
    "\n",
    "def tf_encode(fr, en):\n",
    "    return tf.py_function(encode, [fr, en], [tf.int64, tf.int64])\n",
    "\n",
    "\n",
    "def filter_max_length(fr, en, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(fr) <= max_length,\n",
    "                          tf.size(en) <= max_length)\n",
    "\n",
    "train_dataset = (ds_train\n",
    "                 .map(tf_encode)\n",
    "                 .filter(filter_max_length)\n",
    "                 .cache()\n",
    "                 .shuffle(BUFFER_SIZE)\n",
    "                 .padded_batch(BATCH_SIZE,\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "val_dataset = (ds_val\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150391, 50) (150391, 50)\n",
      "(2219, 50) (2219, 50)\n",
      "(150391, 49) (150391, 49) (2219, 49) (2219, 49)\n"
     ]
    }
   ],
   "source": [
    "ds_train_np = tfds.as_numpy(train_dataset)\n",
    "ds_val_np = tfds.as_numpy(val_dataset)\n",
    "\n",
    "for fr_train, en_train in ds_train_np:\n",
    "    print(en_train.shape, fr_train.shape)\n",
    "    \n",
    "for fr_val, en_val in ds_val_np:\n",
    "    print(en_val.shape, fr_val.shape)\n",
    "\n",
    "# one-step shift between decoder input and output\n",
    "fr_train_input = fr_train[:, :-1]\n",
    "fr_train_output = fr_train[:, 1:]\n",
    "fr_val_input = fr_val[:, :-1]\n",
    "fr_val_output = fr_val[:, 1:]\n",
    "print(fr_train_input.shape, fr_train_output.shape, fr_val_input.shape, fr_val_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150391, 1, 50) (150391, 49, 49) (150391, 1, 50)\n"
     ]
    }
   ],
   "source": [
    "enc_input_mask, dec_input_mask, enc_dec_mask = create_masks(en_train, fr_train_input)\n",
    "print(enc_input_mask.shape, dec_input_mask.shape, enc_dec_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2219, 1, 50) (2219, 49, 49) (2219, 1, 50)\n"
     ]
    }
   ],
   "source": [
    "val_enc_input_mask, val_dec_input_mask, val_enc_dec_mask = create_masks(en_val, fr_val_input)\n",
    "print(val_enc_input_mask.shape, val_dec_input_mask.shape, val_enc_dec_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train our Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1, None)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1, None)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer (Transformer)       ((None, None, 7978), 66802474    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 66,802,474\n",
      "Trainable params: 66,802,474\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "num_heads = 4\n",
    "\n",
    "embedding_dim = 512\n",
    "input_vocab_size = len(vocab_en)\n",
    "target_vocab_size = len(vocab_fr)\n",
    "max_positional_encoding_input = MAX_LENGTH\n",
    "max_positional_encoding_target = MAX_LENGTH\n",
    "\n",
    "fully_connected_dim = 512\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(embedding_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model = build_model(num_layers, num_heads, embedding_dim, fully_connected_dim,\n",
    "                    input_vocab_size, target_vocab_size,\n",
    "                    max_positional_encoding_input, max_positional_encoding_target)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_loss,\n",
    "              optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\venv-dev\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4943: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4700/4700 [==============================] - 880s 185ms/step - loss: 2.4907 - val_loss: 2.1282\n",
      "Epoch 2/100\n",
      "4700/4700 [==============================] - 870s 185ms/step - loss: 1.5302 - val_loss: 1.6588\n",
      "Epoch 3/100\n",
      "4700/4700 [==============================] - 851s 181ms/step - loss: 1.2504 - val_loss: 1.4868\n",
      "Epoch 4/100\n",
      "4700/4700 [==============================] - 851s 181ms/step - loss: 1.1173 - val_loss: 1.4048\n",
      "Epoch 5/100\n",
      "4700/4700 [==============================] - 850s 181ms/step - loss: 1.0320 - val_loss: 1.3678\n",
      "Epoch 6/100\n",
      "4700/4700 [==============================] - 849s 181ms/step - loss: 0.9679 - val_loss: 1.3336\n",
      "Epoch 7/100\n",
      "4700/4700 [==============================] - 849s 181ms/step - loss: 0.9154 - val_loss: 1.3225\n",
      "Epoch 8/100\n",
      "4700/4700 [==============================] - 850s 181ms/step - loss: 0.8714 - val_loss: 1.3140\n",
      "Epoch 9/100\n",
      "4700/4700 [==============================] - 849s 181ms/step - loss: 0.8338 - val_loss: 1.3133\n",
      "Epoch 10/100\n",
      "4700/4700 [==============================] - 848s 180ms/step - loss: 0.8007 - val_loss: 1.3153\n",
      "Epoch 11/100\n",
      "4700/4700 [==============================] - 858s 182ms/step - loss: 0.7708 - val_loss: 1.3220\n",
      "Epoch 12/100\n",
      "4700/4700 [==============================] - 866s 184ms/step - loss: 0.7432 - val_loss: 1.3328\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([en_train, fr_train_input, enc_input_mask, enc_dec_mask, dec_input_mask],\n",
    "                    fr_train_output,\n",
    "                    validation_data=([en_val,fr_val_input,val_enc_input_mask,val_enc_dec_mask,val_dec_input_mask],\n",
    "                                     fr_val_output),\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=False)],\n",
    "                    batch_size=32, epochs=100) # batch_size must be <= BATCH_SIZE, and BATCH_SIZE % batch_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can implement a translation function. The function below is implemented in a simple way. You can enhance it with some techniques you learned, e.g. `beam search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(en_sentence):\n",
    "    en_sentence = [BOS] + list(np.squeeze(en_tokenizer.tokenize([en_sentence]).merge_dims(-2, -1).to_list(), axis=0)) + [EOS]\n",
    "    encoder_input = tf.expand_dims(en_sentence, 0) # only 1 batch\n",
    "    \n",
    "    # decoder start with <BOS>\n",
    "    decoder_input = [BOS]\n",
    "    output = tf.expand_dims(decoder_input, 0) # only 1 batch\n",
    "    \n",
    "    # predict fr output one-by-one\n",
    "    for i in tf.range(MAX_LENGTH):\n",
    "        enc_input_mask, dec_input_mask, enc_dec_mask = create_masks(encoder_input, output)\n",
    "        \n",
    "        # output shape (batch_size, seq_len, vocab_size)\n",
    "        predictions = model.predict([encoder_input,\n",
    "                                     output,\n",
    "                                     enc_input_mask,\n",
    "                                     enc_dec_mask,\n",
    "                                     dec_input_mask])\n",
    "        \n",
    "        # only get last word\n",
    "        predictions = predictions[: , -1:, :]\n",
    "        \n",
    "        # get most posible vocab (or sampling if you like)\n",
    "        pred_idx = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        \n",
    "        # reach <EOS>\n",
    "        if tf.equal(pred_idx, EOS):\n",
    "            break\n",
    "        \n",
    "        output = tf.concat([output, pred_idx], axis=-1)\n",
    "        \n",
    "    # TODO: beam search\n",
    "    \n",
    "    output = tf.squeeze(output, axis=0)\n",
    "    output_words = fr_tokenizer.detokenize([[idx for idx in output if idx > EOS]]) # EOS is the last word in reserved_tokens\n",
    "    return tf.strings.reduce_join(output_words, separator=' ', axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'jane visite en afrique en septembre .']\n"
     ]
    }
   ],
   "source": [
    "en_sentence = \"Jane visits africa in september.\"\n",
    "fr_sentence = translate(en_sentence)\n",
    "print(fr_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
