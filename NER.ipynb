{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\venv\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from official.nlp import optimization as nlp_opt\n",
    "from official.nlp.bert import tokenization as bert_token\n",
    "\n",
    "from berts.berts import BertClassificationModel\n",
    "from berts.utils import get_bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>demons</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>##tra</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>##tors</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sid      token  pos tag\n",
       "0    1  Thousands  NNS   O\n",
       "1    1         of   IN   O\n",
       "2    1     demons  NNS   O\n",
       "3    1      ##tra  NNS   O\n",
       "4    1     ##tors  NNS   O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pre-processed (tokernized) data (original data is from Kaggle)\n",
    "df = pd.read_csv('data/ner/preprocess/ner_case_normal_tokenized.csv', na_filter= False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['tag'].nunique() + 1 # + 1 is for padding and <CLS> <SEP>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_words_seq (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_attention_mask (InputLaye [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segment_mask (InputLayer) [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 108310273   input_words_seq[0][0]            \n",
      "                                                                 input_attention_mask[0][0]       \n",
      "                                                                 input_segment_mask[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 768)    0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 18)     13842       dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 108,324,115\n",
      "Trainable params: 108,324,114\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_url = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2\" # cased is better than uncased in NER\n",
    "model, bert_layer = BertClassificationModel(bert_url, classes, return_sequences=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Thousands, of, demons, ##tra, ##tors, have, m...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-geo, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[They, marched, from, the, Houses, of, Parliam...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Police, put, the, number, of, march, ##ers, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [Thousands, of, demons, ##tra, ##tors, have, m...   \n",
       "1  [Families, of, soldiers, killed, in, the, conf...   \n",
       "2  [They, marched, from, the, Houses, of, Parliam...   \n",
       "3  [Police, put, the, number, of, march, ##ers, a...   \n",
       "4  [The, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                                 tag  \n",
       "0  [O, O, O, O, O, O, O, O, B-geo, O, O, O, O, O,...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-geo, I-geo...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, B-geo, O, O,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_group = df.groupby('sid')\n",
    "sentences = pd.DataFrame(data={'sentence': [list(sentence_group.get_group(g)['token']) for g in sentence_group.groups],\n",
    "                               'tag': [list(sentence_group.get_group(g)['tag']) for g in sentence_group.groups],\n",
    "                              })\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 28996\n"
     ]
    }
   ],
   "source": [
    "# load vocabulary (must be same as pre-trained bert)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "bert_tokenizer = bert_token.FullTokenizer(vocab_file, to_lower_case)\n",
    "print('vocabulary size:', len(bert_tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 1, 'B-geo': 2, 'B-gpe': 3, 'B-per': 4, 'I-geo': 5, 'B-org': 6, 'I-org': 7, 'B-tim': 8, 'B-art': 9, 'I-art': 10, 'I-per': 11, 'I-gpe': 12, 'I-tim': 13, 'B-nat': 14, 'B-eve': 15, 'I-eve': 16, 'I-nat': 17}\n"
     ]
    }
   ],
   "source": [
    "tag2id = {t:i+1 for i, t in enumerate(df['tag'].unique())} # 0 is for padding\n",
    "print(tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim', 'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve', 'I-eve', 'I-nat']\n"
     ]
    }
   ],
   "source": [
    "id2tag = [None]*classes\n",
    "for k, v in tag2id.items():\n",
    "    id2tag[v] = k\n",
    "print(id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_labels = sentences['tag'].map(lambda x: [tag2id[k] for k in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shapes: (38368, 143) (38368, 143) (38368, 143) (38368, 143)\n",
      "validation data shapes: (9591, 143) (9591, 143) (9591, 143) (9591, 143)\n"
     ]
    }
   ],
   "source": [
    "input_words, input_mask, input_seg = get_bert_inputs(bert_tokenizer,\n",
    "                                                     sentences['sentence'],\n",
    "                                                     tokenized=True)\n",
    "labels = tf.ragged.constant(tag_labels).to_tensor()\n",
    "zero_pad = tf.zeros_like(labels[:,:1])\n",
    "labels = tf.concat([zero_pad, labels, zero_pad], axis=-1) # <CLS> <sentence> <SEP>\n",
    "\n",
    "val_size = int(input_words.shape[0] * 0.2)\n",
    "train_input_words, train_input_mask, train_input_seg = input_words[val_size:], input_mask[val_size:], input_seg[val_size:]\n",
    "train_labels = labels[val_size:]\n",
    "valid_input_words, valid_input_mask, valid_input_seg = input_words[:val_size], input_mask[:val_size], input_seg[:val_size]\n",
    "valid_labels = labels[:val_size]\n",
    "print('training data shapes:', train_input_words.shape, train_input_mask.shape, train_input_seg.shape, train_labels.shape)\n",
    "print('validation data shapes:', valid_input_words.shape, valid_input_mask.shape, valid_input_seg.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 3\n",
    "train_data_size = len(train_labels)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp_opt.create_optimizer(2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.math.reduce_sum(loss)/tf.math.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_accuracy(y_true, y_pred):\n",
    "    acc = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), acc.dtype)\n",
    "    acc *= mask\n",
    "    return tf.math.reduce_sum(acc)/tf.math.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tag_loss, optimizer=optimizer, metrics=tag_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1919/1919 [==============================] - 821s 428ms/step - loss: 0.3290 - tag_accuracy: 0.9078 - val_loss: 0.1496 - val_tag_accuracy: 0.9568\n",
      "Epoch 2/3\n",
      "1919/1919 [==============================] - 819s 427ms/step - loss: 0.1189 - tag_accuracy: 0.9620 - val_loss: 0.1417 - val_tag_accuracy: 0.9579\n",
      "Epoch 3/3\n",
      "1919/1919 [==============================] - 817s 426ms/step - loss: 0.0936 - tag_accuracy: 0.9689 - val_loss: 0.1430 - val_tag_accuracy: 0.9588\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_input_words, train_input_mask, train_input_seg], train_labels,\n",
    "                    validation_data=([valid_input_words, valid_input_mask, valid_input_seg], valid_labels),\n",
    "                    batch_size=batch_size, epochs=epochs\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence (tokenized): ['The', 'United', 'States', 'says', 'Iran', 'is', 'trying', 'to', 'covert', '##ly', 'develop', 'nuclear', 'weapons', '.']\n",
      "Prediction:  ['O', 'B-geo', 'I-geo', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Ground true: ['O', 'B-geo', 'I-geo', 'O', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "valid_id = 1234 # 0 ~ 9590\n",
    "pred = model.predict([valid_input_words[valid_id:valid_id+1],\n",
    "                      valid_input_mask[valid_id:valid_id+1],\n",
    "                      valid_input_seg[valid_id:valid_id+1]])\n",
    "pred_tagid = tf.math.argmax(pred, axis=-1)\n",
    "\n",
    "sentence_len = tf.math.reduce_sum(valid_input_mask[valid_id]) - 2\n",
    "pred_tag = [id2tag[id] for id in pred_tagid[0,1:sentence_len+1]]\n",
    "print('Sentence (tokenized):', sentences['sentence'][valid_id])\n",
    "print('Prediction: ', pred_tag)\n",
    "print('Ground true:', [id2tag[id] for id in valid_labels[valid_id,1:sentence_len+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence (tokenized): ['NASA', 'officials', 'now', 'say', 'the', 'shuttle', 'could', 'launch', 'as', 'early', 'as', 'February', '24', '.']\n",
      "Prediction:  ['B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O']\n",
      "Ground true: ['B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-tim', 'I-tim', 'O']\n"
     ]
    }
   ],
   "source": [
    "valid_id = 2234 # 0 ~ 9590\n",
    "pred = model.predict([valid_input_words[valid_id:valid_id+1],\n",
    "                      valid_input_mask[valid_id:valid_id+1],\n",
    "                      valid_input_seg[valid_id:valid_id+1]])\n",
    "pred_tagid = tf.math.argmax(pred, axis=-1)\n",
    "\n",
    "sentence_len = tf.math.reduce_sum(valid_input_mask[valid_id]) - 2\n",
    "pred_tag = [id2tag[id] for id in pred_tagid[0,1:sentence_len+1]]\n",
    "print('Sentence (tokenized):', sentences['sentence'][valid_id])\n",
    "print('Prediction: ', pred_tag)\n",
    "print('Ground true:', [id2tag[id] for id in valid_labels[valid_id,1:sentence_len+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/ner/weights/naive/weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
