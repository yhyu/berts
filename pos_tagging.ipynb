{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from official.nlp import optimization as nlp_opt\n",
    "from official.nlp.bert import tokenization as bert_token\n",
    "\n",
    "from berts.berts import BertClassificationModel\n",
    "from berts.utils import get_bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ner/preprocess/ner_normal_tokenized.csv', na_filter= False)\n",
    "df = df[(df.pos != ':') & (df.pos != ',') & (df.pos != ':') & (df.pos != '``')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['pos'].nunique() + 1 # + 1 is for padding and <CLS> <SEP>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_words_seq (InputLayer)    [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_attention_mask (InputLaye [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segment_mask (InputLayer) [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_words_seq[0][0]            \n",
      "                                                                 input_attention_mask[0][0]       \n",
      "                                                                 input_segment_mask[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 768)    0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 37)     28453       dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,510,694\n",
      "Trainable params: 109,510,693\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "model, bert_layer = BertClassificationModel(bert_url, classes, return_sequences=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>[NNS, IN, NNS, VBN, IN, DT, NN, VBD, DT, NNS, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[they, marched, from, the, houses, of, parliam...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NNS, IN, NN, TO, DT, NN, IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[police, put, the, number, of, marche, ##rs, a...</td>\n",
       "      <td>[NNS, VBD, DT, NN, IN, NNS, NNS, IN, CD, CD, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, protest, comes, on, the, eve, of, the, a...</td>\n",
       "      <td>[DT, NN, VBZ, IN, DT, NN, IN, DT, JJ, NN, IN, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [thousands, of, demonstrators, have, marched, ...   \n",
       "1  [families, of, soldiers, killed, in, the, conf...   \n",
       "2  [they, marched, from, the, houses, of, parliam...   \n",
       "3  [police, put, the, number, of, marche, ##rs, a...   \n",
       "4  [the, protest, comes, on, the, eve, of, the, a...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...  \n",
       "1  [NNS, IN, NNS, VBN, IN, DT, NN, VBD, DT, NNS, ...  \n",
       "2  [PRP, VBD, IN, DT, NNS, IN, NN, TO, DT, NN, IN...  \n",
       "3  [NNS, VBD, DT, NN, IN, NNS, NNS, IN, CD, CD, I...  \n",
       "4  [DT, NN, VBZ, IN, DT, NN, IN, DT, JJ, NN, IN, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_group = df.groupby('sid')\n",
    "sentences = pd.DataFrame(data={'sentence': [list(sentence_group.get_group(g)['token']) for g in sentence_group.groups],\n",
    "                               'pos': [list(sentence_group.get_group(g)['pos']) for g in sentence_group.groups],\n",
    "                              })\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# load vocabulary (must be same as pre-trained bert)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "bert_tokenizer = bert_token.FullTokenizer(vocab_file, to_lower_case)\n",
    "print('vocabulary size:', len(bert_tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NNS': 1, 'IN': 2, 'VBP': 3, 'VBN': 4, 'NNP': 5, 'TO': 6, 'VB': 7, 'DT': 8, 'NN': 9, 'CC': 10, 'JJ': 11, 'VBD': 12, 'WP': 13, 'CD': 14, 'PRP': 15, 'VBZ': 16, 'POS': 17, 'VBG': 18, 'RB': 19, 'WRB': 20, 'PRP$': 21, 'MD': 22, 'WDT': 23, 'JJR': 24, 'JJS': 25, 'WP$': 26, 'RP': 27, 'PDT': 28, 'NNPS': 29, 'EX': 30, 'RBS': 31, 'RBR': 32, 'UH': 33, 'LRB': 34, 'RRB': 35, 'FW': 36}\n"
     ]
    }
   ],
   "source": [
    "pos2id = {p:i+1 for i, p in enumerate(df['pos'].unique())} # 0 is for padding\n",
    "print(pos2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'NNS', 'IN', 'VBP', 'VBN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC', 'JJ', 'VBD', 'WP', 'CD', 'PRP', 'VBZ', 'POS', 'VBG', 'RB', 'WRB', 'PRP$', 'MD', 'WDT', 'JJR', 'JJS', 'WP$', 'RP', 'PDT', 'NNPS', 'EX', 'RBS', 'RBR', 'UH', 'LRB', 'RRB', 'FW']\n"
     ]
    }
   ],
   "source": [
    "id2pos = [None]*classes\n",
    "for k, v in pos2id.items():\n",
    "    id2pos[v] = k\n",
    "print(id2pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = sentences['pos'].map(lambda x: [pos2id[k] for k in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shapes: (38367, 110) (38367, 110) (38367, 110) (38367, 110)\n",
      "validation data shapes: (9591, 110) (9591, 110) (9591, 110) (9591, 110)\n"
     ]
    }
   ],
   "source": [
    "input_words, input_mask, input_seg = get_bert_inputs(bert_tokenizer,\n",
    "                                                     sentences['sentence'],\n",
    "                                                     tokenized=True)\n",
    "labels = tf.ragged.constant(pos_labels).to_tensor()\n",
    "zero_pad = tf.zeros_like(labels[:,:1])\n",
    "labels = tf.concat([zero_pad, labels, zero_pad], axis=-1) # <CLS> <sentence> <SEP>\n",
    "\n",
    "val_size = int(input_words.shape[0] * 0.2)\n",
    "train_input_words, train_input_mask, train_input_seg = input_words[val_size:], input_mask[val_size:], input_seg[val_size:]\n",
    "train_labels = labels[val_size:]\n",
    "valid_input_words, valid_input_mask, valid_input_seg = input_words[:val_size], input_mask[:val_size], input_seg[:val_size]\n",
    "valid_labels = labels[:val_size]\n",
    "print('training data shapes:', train_input_words.shape, train_input_mask.shape, train_input_seg.shape, train_labels.shape)\n",
    "print('validation data shapes:', valid_input_words.shape, valid_input_mask.shape, valid_input_seg.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "epochs = 3\n",
    "train_data_size = len(train_labels)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp_opt.create_optimizer(2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.math.reduce_sum(loss)/tf.math.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_accuracy(y_true, y_pred):\n",
    "    acc = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), acc.dtype)\n",
    "    acc *= mask\n",
    "    return tf.math.reduce_sum(acc)/tf.math.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=pos_loss, optimizer=optimizer, metrics=pos_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1599/1599 [==============================] - 619s 387ms/step - loss: 0.4315 - pos_accuracy: 0.8849 - val_loss: 0.0838 - val_pos_accuracy: 0.9755\n",
      "Epoch 2/3\n",
      "1599/1599 [==============================] - 619s 387ms/step - loss: 0.0761 - pos_accuracy: 0.9763 - val_loss: 0.0726 - val_pos_accuracy: 0.9776\n",
      "Epoch 3/3\n",
      "1599/1599 [==============================] - 617s 386ms/step - loss: 0.0588 - pos_accuracy: 0.9809 - val_loss: 0.0722 - val_pos_accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([train_input_words, train_input_mask, train_input_seg], train_labels,\n",
    "                    validation_data=([valid_input_words, valid_input_mask, valid_input_seg], valid_labels),\n",
    "                    batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  ['DT', 'JJ', 'JJ', 'NN', 'VBZ', 'VBN', 'NNP', 'POS', 'POS', 'NN', 'IN', 'NNS', 'NNS']\n",
      "ground true: ['DT', 'JJ', 'JJ', 'NN', 'VBZ', 'VBN', 'NNP', 'POS', 'POS', 'NN', 'IN', 'NNS', 'NNS']\n"
     ]
    }
   ],
   "source": [
    "valid_id = 1000 # 0 ~ 9590\n",
    "pred = model.predict([valid_input_words[valid_id:valid_id+1],\n",
    "                      valid_input_mask[valid_id:valid_id+1],\n",
    "                      valid_input_seg[valid_id:valid_id+1]])\n",
    "pred_posid = tf.math.argmax(pred, axis=-1)\n",
    "\n",
    "sentence_len = tf.math.reduce_sum(valid_input_mask[valid_id]) - 2\n",
    "pred_pos = [id2pos[id] for id in pred_posid[0,1:sentence_len+1]]\n",
    "print('prediction: ', pred_pos)\n",
    "print('ground true:', [id2pos[id] for id in valid_labels[valid_id,1:sentence_len+1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
